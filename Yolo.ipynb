{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c25fab2-304a-40de-9853-9d8560db256a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b9bc07-a01b-4d47-8b2f-d4f7877d32c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d19fea5-398d-412d-8694-59f4672d9667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8m-cls.pt', task='classification')  # build a new model from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c855a5-60ec-42dc-a12c-2645e542ade1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.206  Python-3.11.3 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3070 Ti Laptop GPU, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8m-cls.pt, data=./prepared_data/, epochs=5, patience=50, batch=16, imgsz=[128, 640], save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train6\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\train... found 539 images in 4 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\val... found 169 images in 4 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\test... found 206 images in 4 classes  \n",
      "Overriding model.yaml nc=1000 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n",
      "  8                  -1  2   7084032  ultralytics.nn.modules.block.C2f             [768, 768, 2, True]           \n",
      "  9                  -1  1    990724  ultralytics.nn.modules.head.Classify         [768, 4]                      \n",
      "YOLOv8m-cls summary: 141 layers, 15777460 parameters, 15777460 gradients, 41.9 GFLOPs\n",
      "Transferred 228/230 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\classify\\train6', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "WARNING  updating to 'imgsz=640'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\train... 539 images, 0 corrupt: 100%|████████\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\val... 169 images, 0 corrupt: 100%|██████████| \u001b[0m\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 38 weight(decay=0.0), 39 weight(decay=0.0005), 39 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train6\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "        1/5      4.89G     0.3182         11        640: 100%|██████████| 34/34 [00:13<00:00,  2.57it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:01<00:00,  5.83it/s]\n",
      "                   all      0.527          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "        2/5         5G     0.1924         11        640: 100%|██████████| 34/34 [00:09<00:00,  3.51it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00,  9.97it/s]\n",
      "                   all      0.503          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "        3/5      5.02G    0.08957         11        640: 100%|██████████| 34/34 [00:08<00:00,  3.79it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00,  9.60it/s]\n",
      "                   all      0.485          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "        4/5      5.04G    0.03398         11        640: 100%|██████████| 34/34 [00:08<00:00,  4.14it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00,  9.69it/s]\n",
      "                   all      0.479          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "        5/5      5.02G    0.01642         11        640: 100%|██████████| 34/34 [00:08<00:00,  3.88it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00,  9.89it/s]\n",
      "                   all      0.473          1\n",
      "\n",
      "5 epochs completed in 0.018 hours.\n",
      "Optimizer stripped from runs\\classify\\train6\\weights\\last.pt, 31.7MB\n",
      "Optimizer stripped from runs\\classify\\train6\\weights\\best.pt, 31.7MB\n",
      "\n",
      "Validating runs\\classify\\train6\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.206  Python-3.11.3 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3070 Ti Laptop GPU, 8192MiB)\n",
      "YOLOv8m-cls summary (fused): 103 layers, 15767780 parameters, 0 gradients, 41.6 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\train... found 539 images in 4 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\val... found 169 images in 4 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\test... found 206 images in 4 classes  \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:01<00:00,  4.70it/s]\n",
      "                   all      0.527          1\n",
      "Speed: 2.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train6\u001b[0m\n",
      "Results saved to \u001b[1mruns\\classify\\train6\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000001BB5A986050>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.7633136212825775\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.526627242565155, 'metrics/accuracy_top5': 1.0, 'fitness': 0.7633136212825775}\n",
       "save_dir: WindowsPath('runs/classify/train6')\n",
       "speed: {'preprocess': 2.1021140397653073, 'inference': 4.151809850388025, 'loss': 0.0, 'postprocess': 0.0}\n",
       "task: 'classify'\n",
       "top1: 0.526627242565155\n",
       "top5: 1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(data=\"./prepared_data/\", epochs=5, imgsz=[128, 640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac0212b-3b80-4de4-bab7-0bb53c3801fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.children at 0x000001BB56732420>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4d1b61-9ed6-4c01-a50b-028db0a3ce30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential()\n"
     ]
    }
   ],
   "source": [
    "newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "print(newmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a0c40a2-3ebf-4969-a5e9-11b3f2e8f836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(\n",
       "  (conv): Conv2d(768, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act): SiLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(list(list(model.children())[0].children())[0].children())[-1].children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "34927b2c-40bb-44a1-8331-aec544fcc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_new = children[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7da78c3b-6606-4374-a7ca-55228ecbf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_new.append(list(children[-1].children())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "00d5db86-9702-4ede-b8ff-39aad1d789b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = torch.nn.Sequential(*ch_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "429e5fbe-ca77-4c3c-b441-2d78ce88d109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(\n",
       "  (conv): Conv2d(768, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (act): SiLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(children[-1].children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "76ce885c-a818-44a9-ab72-cb852fb4edaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.nn.modules.container.Sequential() argument after * must be an iterable, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: torch.nn.modules.container.Sequential() argument after * must be an iterable, not NoneType"
     ]
    }
   ],
   "source": [
    "torch.nn.Sequential(*children[:-1].append(list(children[-1].children())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3f2f71cb-a3c5-4d9a-b515-364f2004e10e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics.data.dataset import ClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f238da7d-b0ff-45cd-b860-8dcb2e1c747b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ClassificationDataset.__init__() got an unexpected keyword argument 'cache_ram'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprepared_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_ram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ClassificationDataset.__init__() got an unexpected keyword argument 'cache_ram'"
     ]
    }
   ],
   "source": [
    "ClassificationDataset('prepared_data', cache_ram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fa25f2a-94b9-4bc4-86e7-66c3d0ca3d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ClassificationDataset.__init__() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cl \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprepared_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ClassificationDataset.__init__() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "cl = ClassificationDataset(root=\"prepared_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "313c5e3e-a3f4-4a38-a8b5-c5009c2e945f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics.data.loaders import LoadImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6bf8e79e-60b4-4eb7-a334-91e07161c2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs = LoadImages('prepared_data/val/сэйнэн', imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4947f777-94c0-4280-b718-9af2515ecda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in imgs:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "12bc5658-748a-476a-87f8-2cd02529b845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2105, 3)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "316553dc-c929-4484-ab6a-26bdcc35455a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnew_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:40\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[31;1mtuple of (int, int)\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "new_model(i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6b5fff42-6429-4bec-9b61-602e6ad1b45d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = torch.rand(([1, 3, 128, 640])).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04cd94-f755-4327-ab6b-7b04025bd98e",
   "metadata": {
    "tags": []
   },
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fc0ee6f2-ab26-4e5f-af7b-1c2253a3479a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mama = new_model(inp).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "525b6eba-174b-4c28-b2d6-466d627eccf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 4, 20])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bc6c1f23-a2a3-4cfa-94a7-5bf0327ffcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): ClassificationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-3): 4 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-3): 4 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Classify(\n",
       "        (conv): Conv(\n",
       "          (conv): Conv2d(768, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (drop): Dropout(p=0.0, inplace=True)\n",
       "        (linear): Linear(in_features=1280, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b329f01-ea16-4cf6-a20e-b5bd36071247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open('C:/Users/Pavel/Desktop/ACV/IU-acv-manga-rec/data/сёдзё/Приёмная дочь протагониста/0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d59a35d6-8c73-4959-b963-213ccf2d10aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a8823a1f-778a-4de7-afd4-a90bd2a9d8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "comp = transforms.Compose([\n",
    "        transforms.Resize((128,640)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7d031654-c6f1-45fa-b0c1-ebb70788d5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed = comp(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "233f1105-873a-42c4-a2e1-de1d4a2b8cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 640])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bc54cbf0-42e9-4fb4-b97a-6f4db42ab451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 640])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3d67110d-502e-493e-b8a3-5c2d6b8aed3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = new_model(transformed.unsqueeze(0).to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "13a03b9d-3446-4e9e-8b18-35bb5dab698e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 4, 20])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2192ddc6-03e1-44c7-978b-e8296a35e170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "sas = {1: 2, 3: 4}\n",
    "\n",
    "for s in sas.items():\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf1674b-0265-4c1c-b800-636b9489ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a795bc2-3c97-4fc4-b18c-e329121c6300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3605db1a-f559-4f27-822c-fca25ed9661e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = [128,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6beb6c9-5a13-4f71-b80d-11f96fa785da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, path: str, shape: list[int]):\n",
    "        classes = os.listdir(path)\n",
    "        res = []\n",
    "        CLASSES = {\n",
    "            \"дзёсэй\": 0, \n",
    "            \"сёдзё\": 1, \n",
    "            \"сёнэн\": 2, \n",
    "            \"сэйнэн\": 3\n",
    "        }\n",
    "        for genre, cl in CLASSES.items():\n",
    "            images = os.listdir(os.path.join(path, genre))\n",
    "            res.extend(list(map(lambda x: (os.path.join(path, genre, x), cl), images)))\n",
    "        shuffle(res)\n",
    "        self.items = res\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])\n",
    "        self.comp = transforms.Compose([\n",
    "            transforms.Resize(shape),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, cl = self.items[idx]\n",
    "        im = Image.open(path).convert(\"RGB\")\n",
    "        return (self.comp(im), torch.Tensor(np.array([cl])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6828d1e-105d-4552-b924-3cc603a4b5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sas = YoloDataset(\"prepared_data/train/\", SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "514dd1c3-3519-43a7-93da-d2feb32b9bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(sas, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38555319-6681-4c87-afb7-6bb9b1753d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592c884-3370-4d15-bd73-c5861556f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloModel:\n",
    "    def __init__(self, path, shape):\n",
    "        model = YOLO('yolov8m-cls.pt', task='classification')\n",
    "        model.train(data=path, epochs=10, imgsz=shape)\n",
    "        children = list(list(list(list(model.children())[0].children())[0].children()))\n",
    "        ch_new = children[:-1]\n",
    "        self.model = torch.nn.Sequential(*ch_new)\n",
    "        \n",
    "        \n",
    "    def \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4028fa7-3ece-49df-97aa-fca7fa9b36e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.206  Python-3.11.3 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3070 Ti Laptop GPU, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8m-cls.pt, data=./prepared_data/, epochs=10, patience=50, batch=16, imgsz=[128, 512], save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train9\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\train... found 539 images in 4 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\val... found 169 images in 4 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\test... found 206 images in 4 classes  \n",
      "Overriding model.yaml nc=1000 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n",
      "  8                  -1  2   7084032  ultralytics.nn.modules.block.C2f             [768, 768, 2, True]           \n",
      "  9                  -1  1    990724  ultralytics.nn.modules.head.Classify         [768, 4]                      \n",
      "YOLOv8m-cls summary: 141 layers, 15777460 parameters, 15777460 gradients, 41.9 GFLOPs\n",
      "Transferred 228/230 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\classify\\train9', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "WARNING  updating to 'imgsz=512'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\train... 539 images, 0 corrupt: 100%|████████\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\val... 169 images, 0 corrupt: 100%|██████████| \u001b[0m\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 38 weight(decay=0.0), 39 weight(decay=0.0005), 39 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train9\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       1/10      2.93G      0.318         11        512: 100%|██████████| 34/34 [00:08<00:00,  3.97it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 10.29it/s]\n",
      "                   all      0.521          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       2/10      3.09G     0.1825         11        512: 100%|██████████| 34/34 [00:09<00:00,  3.57it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 15.26it/s]\n",
      "                   all      0.503          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       3/10       3.1G    0.07758         11        512: 100%|██████████| 34/34 [00:09<00:00,  3.59it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 15.19it/s]\n",
      "                   all      0.497          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       4/10      3.08G    0.02498         11        512: 100%|██████████| 34/34 [00:10<00:00,  3.36it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 15.17it/s]\n",
      "                   all      0.408          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       5/10      3.08G    0.01425         11        512: 100%|██████████| 34/34 [00:09<00:00,  3.52it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 15.18it/s]\n",
      "                   all      0.467          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       6/10      3.09G    0.02672         11        512: 100%|██████████| 34/34 [00:09<00:00,  3.63it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 15.31it/s]\n",
      "                   all      0.456          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       7/10      3.08G    0.02803         11        512: 100%|██████████| 34/34 [00:09<00:00,  3.58it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 15.21it/s]\n",
      "                   all      0.462          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       8/10       3.1G     0.0189         11        512: 100%|██████████| 34/34 [00:09<00:00,  3.41it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 15.17it/s]\n",
      "                   all      0.396          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       9/10      3.08G    0.01685         11        512: 100%|██████████| 34/34 [00:10<00:00,  3.14it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 14.89it/s]\n",
      "                   all      0.432          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      10/10      3.25G    0.01058         11        512: 100%|██████████| 34/34 [00:09<00:00,  3.63it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00, 15.13it/s]\n",
      "                   all      0.414          1\n",
      "\n",
      "10 epochs completed in 0.041 hours.\n",
      "Optimizer stripped from runs\\classify\\train9\\weights\\last.pt, 31.7MB\n",
      "Optimizer stripped from runs\\classify\\train9\\weights\\best.pt, 31.7MB\n",
      "\n",
      "Validating runs\\classify\\train9\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.206  Python-3.11.3 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3070 Ti Laptop GPU, 8192MiB)\n",
      "YOLOv8m-cls summary (fused): 103 layers, 15767780 parameters, 0 gradients, 41.6 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\train... found 539 images in 4 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\val... found 169 images in 4 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\Pavel\\Desktop\\ACV\\IU-acv-manga-rec\\prepared_data\\test... found 206 images in 4 classes  \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 6/6 [00:00<00:00,  7.44it/s]\n",
      "                   all      0.527          1\n",
      "Speed: 1.2ms preprocess, 2.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train9\u001b[0m\n",
      "Results saved to \u001b[1mruns\\classify\\train9\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002AD06BB0DD0>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.7633136212825775\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.526627242565155, 'metrics/accuracy_top5': 1.0, 'fitness': 0.7633136212825775}\n",
       "save_dir: WindowsPath('runs/classify/train9')\n",
       "speed: {'preprocess': 1.2326438046066013, 'inference': 2.771536979449571, 'loss': 0.0, 'postprocess': 0.0}\n",
       "task: 'classify'\n",
       "top1: 0.526627242565155\n",
       "top5: 1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8m-cls.pt', task='classification')  # build a new model from YAML\n",
    "model.train(data=\"./prepared_data/\", epochs=10, imgsz=SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07ff332-ec20-4904-a51a-ab9ce4ed9388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "children = list(list(list(list(model.children())[0].children())[0].children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b8b03bc-1308-401e-9bee-3d4e4a185351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ch_new = children[:-1]\n",
    "# ch_new.append(list(children[-2].children())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d84f4cb1-eb91-436c-a97b-27961453d064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = torch.nn.Sequential(*ch_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f936115c-5204-493d-a624-74f91a63f4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = new_model(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97d2a96f-bc55-4a88-a04a-6726218a0cab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 49152])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.flatten(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cc746b0-d5f5-4ff8-8f9b-eba0b214581d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_features = []\n",
    "res_classes = []\n",
    "\n",
    "res = list(map(lambda x: (new_model(x[0]).flatten(1), x[1]), train_dataloader))\n",
    "\n",
    "# for features, classes in train_dataloader:\n",
    "#     res = new_model(train_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2a73c9f1-d5a7-4d60-b227-a8769228792d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectors, classes = functools.reduce(lambda x, y: (torch.cat((x[0], y[0])), torch.cat((x[1], y[1]))), res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e8847b7c-b581-4f26-8ac8-0c1c28ecd3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([539, 49152])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "22fd4674-924f-4dfa-99f7-c8ea14584167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_np = vectors.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "06af9d75-9df8-4581-8046-f730687c5ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "182f6208-abef-410f-bdfc-75bb3b83a3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pcaed = PCA(2).fit_transform(res_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5b0d16c2-a7f4-49b1-a153-33747c2141a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(539, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcaed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0988542a-8500-49ea-b9d5-236274b29832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sas = list(zip(pcaed, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "77bfd2bb-5239-473f-b876-fd2bc72b85d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74112dc0-358d-4024-b680-af2b77b82483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "caf48183-755b-4e83-9aca-e4f3cfbab634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for s, cl in sas:\n",
    "    # print(s[0], s[1])\n",
    "    if cl.numpy()[0] == 0:\n",
    "        plt.plot(s[0], s[1], 'ro')\n",
    "    elif cl.numpy()[0] == 1:\n",
    "        plt.plot(s[0], s[1], 'bo')\n",
    "    elif cl.numpy()[0] == 2:\n",
    "        plt.plot(s[0], s[1], 'go')\n",
    "    elif cl.numpy()[0] == 3:\n",
    "        plt.plot(s[0], s[1], 'co')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4746912-8172-4868-b5d9-fc68780e856b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
